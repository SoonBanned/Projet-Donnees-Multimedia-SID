{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5199e967",
   "metadata": {},
   "source": [
    "# Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b63a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import av # PyAV library for efficient video I/O\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T # Used for frame preprocessing\n",
    "import torchvision.models as models # Needed for R(2+1)D model\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.video import R2Plus1D_18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e4bcd3",
   "metadata": {},
   "source": [
    "# Extraction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The expected feature dimension from the R(2+1)D-18 model without the final FC layer.\n",
    "VIDEO_FEATURE_DIM = 512\n",
    "TARGET_CLIP_LENGTH = 16\n",
    "TARGET_H, TARGET_W = 112, 112\n",
    "\n",
    "# Global feature extractor instance for lazy loading\n",
    "_r2plus1d_feature_extractor = None\n",
    "\n",
    "def get_r2plus1d_feature_extractor():\n",
    "    \"\"\"\n",
    "    Lazily loads the pre-trained R(2+1)D-18 model and modifies it \n",
    "    to output the 512-dimensional features.\n",
    "    \"\"\"\n",
    "    global _r2plus1d_feature_extractor\n",
    "    if _r2plus1d_feature_extractor is None:\n",
    "        print(\"Loading pre-trained r2plus1d_18 model for feature extraction...\")\n",
    "        # Load the R(2+1)D-18 model pre-trained on Kinetics-400\n",
    "        model = models.video.r2plus1d_18(weights=R2Plus1D_18_Weights.KINETICS400_V1) \n",
    "        \n",
    "        # Modify the last fully connected layer (fc) to be an Identity function\n",
    "        model.fc = nn.Identity()\n",
    "        \n",
    "        model.eval() # Set to evaluation mode\n",
    "        _r2plus1d_feature_extractor = model.cuda() if torch.cuda.is_available() else model\n",
    "        print(f\"R(2+1)D Feature Extractor loaded on device: {_r2plus1d_feature_extractor.device}\")\n",
    "\n",
    "    return _r2plus1d_feature_extractor\n",
    "\n",
    "def extract_features_from_video(video_path: str) -> Tuple[torch.Tensor, str]:\n",
    "    \"\"\"\n",
    "    Extracts R(2+1)D features from a raw video file using PyAV.\n",
    "    Returns the feature sequence tensor and the output path for saving.\n",
    "    \"\"\"\n",
    "    feature_extractor = get_r2plus1d_feature_extractor()\n",
    "    cu = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(cu)\n",
    "    # Define the output path for the feature file\n",
    "    feature_save_path = video_path.rsplit('.', 1)[0] + '_features.npy'\n",
    "    \n",
    "    # Skip extraction if the feature file already exists\n",
    "    if os.path.exists(feature_save_path):\n",
    "        print(f\"  --> Features already exist: {os.path.basename(feature_save_path)}. Skipping.\")\n",
    "        return None, feature_save_path\n",
    "    \n",
    "    print(f\"  --> Starting extraction for {os.path.basename(video_path)}\")\n",
    "\n",
    "    # Preprocessing transforms\n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((TARGET_H, TARGET_W)),\n",
    "        T.ToTensor(), \n",
    "        T.Normalize(\n",
    "            mean=[0.43216, 0.394666, 0.37645], # Kinetics-400 means\n",
    "            std=[0.22803, 0.221459, 0.216328]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    all_frames = []\n",
    "    try:\n",
    "        with av.open(video_path) as container:\n",
    "            stream = container.streams.video[0]\n",
    "            \n",
    "            for frame in container.decode(stream):\n",
    "                img = frame.to_rgb().to_image()\n",
    "                tensor_frame = preprocess(img)\n",
    "                all_frames.append(tensor_frame)\n",
    "\n",
    "        if not all_frames:\n",
    "            raise RuntimeError(\"Could not decode any frames from the video.\")\n",
    "\n",
    "        all_frames_tensor = torch.stack(all_frames).float()\n",
    "        total_frames = all_frames_tensor.size(0)\n",
    "\n",
    "    except av.AVError as e:\n",
    "        warnings.warn(f\"PyAV failed to load video {os.path.basename(video_path)}: {e}. Skipping.\", UserWarning)\n",
    "        return None, feature_save_path\n",
    "    \n",
    "    # --- Temporal sampling and feature extraction ---\n",
    "\n",
    "    if total_frames < TARGET_CLIP_LENGTH:\n",
    "        warnings.warn(f\"Video {os.path.basename(video_path)} is too short ({total_frames} frames). Skipping extraction.\", UserWarning)\n",
    "        return None, feature_save_path \n",
    "\n",
    "    NUM_CLIPS_TO_EXTRACT = total_frames // TARGET_CLIP_LENGTH\n",
    "    trimmed_frames = all_frames_tensor[:NUM_CLIPS_TO_EXTRACT * TARGET_CLIP_LENGTH]\n",
    "    \n",
    "    # Reshape and permute to R(2+1)D format: (N, C, T, H, W)\n",
    "    video_clips = trimmed_frames.view(NUM_CLIPS_TO_EXTRACT, TARGET_CLIP_LENGTH, 3, TARGET_H, TARGET_W)\n",
    "    video_clips = video_clips.permute(0, 2, 1, 3, 4).to(device)\n",
    "    \n",
    "    all_clip_features = []\n",
    "    with torch.no_grad():\n",
    "        # Process clips in batches to save memory/time if there are many clips\n",
    "        clip_batch_size = 32 \n",
    "        for i in range(0, video_clips.size(0), clip_batch_size):\n",
    "            clip_batch = video_clips[i:i + clip_batch_size]\n",
    "            \n",
    "            # feature_vector shape: (batch_size, 512)\n",
    "            feature_vector = feature_extractor(clip_batch) \n",
    "            all_clip_features.append(feature_vector)\n",
    "\n",
    "    # Concatenate all clip features to get the sequence tensor (num_clips, 512)\n",
    "    video_features_sequence = torch.cat(all_clip_features, dim=0).cpu() # Move back to CPU for numpy saving\n",
    "    \n",
    "    return video_features_sequence, feature_save_path\n",
    "\n",
    "def extract(data_root_dir: str):\n",
    "    \"\"\"\n",
    "    Walks through the data root directory, finds .mp4 files, extracts features, \n",
    "    and saves them as .npy files.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(data_root_dir):\n",
    "        print(f\"Error: Directory not found at {data_root_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting batch feature extraction from: {data_root_dir}\")\n",
    "    \n",
    "    video_files = []\n",
    "    # Traverse the directory structure (assuming class folders are immediate subdirectories)\n",
    "    for dirpath, dirnames, filenames in os.walk(data_root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith(('.mp4', '.avi', '.mov', '.webm')):\n",
    "                video_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "    if not video_files:\n",
    "        print(f\"No video files found in {data_root_dir}. Ensure they are in class subfolders.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_files)} video files to process.\")\n",
    "    \n",
    "    # Perform extraction for all found videos\n",
    "    for video_path in video_files:\n",
    "        try:\n",
    "            features, save_path = extract_features_from_video(video_path)\n",
    "            \n",
    "            if features is not None:\n",
    "                # Save the features using NumPy\n",
    "                np.save(save_path, features.numpy())\n",
    "                print(f\"  -- Successfully saved features of shape {features.shape} to: {os.path.basename(save_path)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred processing {os.path.basename(video_path)}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ecb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract(\"./TrainValVideo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941480f6",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79de639",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FEATURE_DIM = 512 # Matches the output dimension of R(2+1)D features\n",
    "NUM_CLASSES = 2 # Default number of classes, will be updated by setup_data_pipeline\n",
    "# Use a module-level variable for lazy loading of the feature extractor\n",
    "_r2plus1d_feature_extractor = None\n",
    "\n",
    "def get_r2plus1d_feature_extractor():\n",
    "    \"\"\"\n",
    "    Lazily loads the pre-trained R(2+1)D-34 model and modifies it \n",
    "    to output the 2048-dimensional features.\n",
    "    \"\"\"\n",
    "    global _r2plus1d_feature_extractor\n",
    "    if _r2plus1d_feature_extractor is None:\n",
    "        print(\"Loading pre-trained r2plus1d_34 model for feature extraction...\")\n",
    "        # Load the R(2+1)D-34 model pre-trained on Kinetics-400\n",
    "        # NOTE: We use r2plus1d_18 from torchvision which is a standard choice.\n",
    "        # It is highly recommended to use the best performing and most stable version available.\n",
    "        model = models.video.r2plus1d_18(pretrained=True) \n",
    "        \n",
    "        # Modify the last fully connected layer (fc) to be an Identity function\n",
    "        # This allows us to access the 2048-dimensional output of the Global Average Pooling layer.\n",
    "        model.fc = nn.Identity()\n",
    "        \n",
    "        model.eval() # Set to evaluation mode\n",
    "        _r2plus1d_feature_extractor = model\n",
    "        print(f\"R(2+1)D Feature Extractor loaded. Output dimension: {VIDEO_FEATURE_DIM}\")\n",
    "\n",
    "    return _r2plus1d_feature_extractor\n",
    "\n",
    "\n",
    "# --- 1. PyTorch Custom Dataset for Video Classification ---\n",
    "\n",
    "class VideoClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset to load either pre-extracted features (.npy, .pt) or \n",
    "    raw video files (.mp4) and their corresponding labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root: str, extensions: List[str] = ['.npy', '.pt', '.mp4']):\n",
    "        self.data_root = data_root\n",
    "        self.extensions = extensions\n",
    "        self.samples = [] # Stores list of (file_filepath, class_label_index)\n",
    "        self.class_to_idx: Dict[str, int] = {}\n",
    "        \n",
    "        # 1. Index the dataset\n",
    "        self._index_dataset()\n",
    "\n",
    "    def _index_dataset(self):\n",
    "        \"\"\"Walks the data root to find all feature/video files and assign labels.\"\"\"\n",
    "        \n",
    "        # Map class names (folder names) to integer indices\n",
    "        class_names = [d.name for d in os.scandir(self.data_root) if d.is_dir()]\n",
    "        class_names.sort() # Ensure consistent mapping\n",
    "        \n",
    "        for i, class_name in enumerate(class_names):\n",
    "            self.class_to_idx[class_name] = i\n",
    "            class_dir = os.path.join(self.data_root, class_name)\n",
    "            \n",
    "            for entry in os.scandir(class_dir):\n",
    "                if entry.is_file() and any(entry.name.endswith(ext) for ext in self.extensions):\n",
    "                    # Add the sample: (filepath, class_index)\n",
    "                    self.samples.append((entry.path, i))\n",
    "\n",
    "        print(f\"Found {len(self.samples)} files across {len(self.class_to_idx)} classes.\")\n",
    "        print(f\"Class Mapping: {self.class_to_idx}\")\n",
    "        \n",
    "    def _extract_features_from_video(self, video_path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extracts R(2+1)D features from a raw video file.\n",
    "        \n",
    "        CRITICAL NOTE: This method simulates video loading and preprocessing.\n",
    "        You MUST integrate a video library (PyAV recommended for speed) to \n",
    "        replace the simulated I/O and preprocessing steps below.\n",
    "        \n",
    "        The final output MUST be a tensor of shape (num_clips, VIDEO_FEATURE_DIM).\n",
    "        \"\"\"\n",
    "\n",
    "        # --- ACTUAL R(2+1)D MODEL LOADING ---\n",
    "        feature_extractor = get_r2plus1d_feature_extractor()\n",
    "        \n",
    "        # --- PLACEHOLDER: Video I/O, Resizing, and Frame Sampling ---\n",
    "        \n",
    "        TARGET_CLIP_LENGTH = 16\n",
    "        TARGET_H, TARGET_W = 112, 112\n",
    "        \n",
    "        all_frames = []\n",
    "        \n",
    "        # Define the necessary preprocessing transforms for R(2+1)D \n",
    "        # (Resize to 112x112 and normalize with Kinetics-400 mean/std)\n",
    "        preprocess = T.Compose([\n",
    "            T.Resize((TARGET_H, TARGET_W)), # Resize to 112x112 (no cropping)\n",
    "            T.ToTensor(), # Converts PIL to Tensor C,H,W, float [0, 1]\n",
    "            T.Normalize(\n",
    "                mean=[0.43216, 0.394666, 0.37645], # Kinetics-400 means\n",
    "                std=[0.22803, 0.221459, 0.216328]   # Kinetics-400 stds\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            with av.open(video_path) as container:\n",
    "                # Get the first video stream\n",
    "                stream = container.streams.video[0]\n",
    "                \n",
    "                # Iterate through all frames\n",
    "                for frame in container.decode(stream):\n",
    "                    # Convert AVFrame to a standard format (RGB, Image) and apply preprocessing\n",
    "                    img = frame.to_rgb().to_image()\n",
    "                    tensor_frame = preprocess(img)\n",
    "                    all_frames.append(tensor_frame)\n",
    "\n",
    "            if not all_frames:\n",
    "                raise RuntimeError(\"Could not decode any frames from the video.\")\n",
    "\n",
    "            # Stack all frames: (Total_Frames, C, H, W) -> (Total_Frames, 3, 112, 112)\n",
    "            all_frames_tensor = torch.stack(all_frames).float()\n",
    "            total_frames = all_frames_tensor.size(0)\n",
    "\n",
    "        except av.AVError as e:\n",
    "            warnings.warn(f\"PyAV failed to load video {os.path.basename(video_path)}: {e}. Returning zero tensor.\", UserWarning)\n",
    "            total_frames = 0\n",
    "            all_frames_tensor = torch.empty(0)\n",
    "            \n",
    "        # --- 2. Dense, Non-overlapping temporal sampling ---\n",
    "        \n",
    "        if total_frames < TARGET_CLIP_LENGTH:\n",
    "            # Handle case where video is too short\n",
    "            warnings.warn(f\"Video {os.path.basename(video_path)} is too short ({total_frames} frames). Returning single zero feature vector.\", UserWarning)\n",
    "            # This returns a single zero feature vector, which is then padded later.\n",
    "            return torch.zeros(1, VIDEO_FEATURE_DIM) \n",
    "\n",
    "        # Calculate the maximum number of full, non-overlapping 16-frame clips.\n",
    "        NUM_CLIPS_TO_EXTRACT = total_frames // TARGET_CLIP_LENGTH\n",
    "        \n",
    "        # Slice the frames to only include the clips we can form \n",
    "        trimmed_frames = all_frames_tensor[:NUM_CLIPS_TO_EXTRACT * TARGET_CLIP_LENGTH]\n",
    "        \n",
    "        # Reshape trimmed frames into clips:\n",
    "        # (N*16, 3, H, W) -> (N, 16, 3, H, W)\n",
    "        video_clips = trimmed_frames.view(NUM_CLIPS_TO_EXTRACT, TARGET_CLIP_LENGTH, TARGET_H, TARGET_W, 3) # N, T, H, W, C\n",
    "        # Permute to the R(2+1)D required format: (N, C, T, H, W)\n",
    "        video_clips = video_clips.permute(0, 4, 1, 2, 3) # N, C, T, H, W\n",
    "        \n",
    "        if NUM_CLIPS_TO_EXTRACT == 0:\n",
    "            # Handle case where video is too short (less than 16 frames)\n",
    "            warnings.warn(f\"Video {os.path.basename(video_path)} is too short ({total_frames} frames) to extract a full 16-frame clip. Using a single zero-padded clip.\", UserWarning)\n",
    "            # Create a single zero-padded clip (1, 3, 16, 112, 112)\n",
    "            video_clips = torch.zeros(1, 3, TARGET_CLIP_LENGTH, TARGET_H, TARGET_W).float()\n",
    "            # Note: A real implementation might sample the available frames and pad the rest.\n",
    "        else:\n",
    "            # Slice the frames to only include the clips we can form (e.g., 150 -> 144 frames for 9 clips)\n",
    "            trimmed_frames = all_frames_tensor[:NUM_CLIPS_TO_EXTRACT * TARGET_CLIP_LENGTH]\n",
    "            \n",
    "            # Reshape trimmed frames into clips:\n",
    "            # (N*16, 3, H, W) -> (N, 16, 3, H, W)\n",
    "            video_clips = trimmed_frames.view(NUM_CLIPS_TO_EXTRACT, TARGET_CLIP_LENGTH, 3, TARGET_H, TARGET_W)\n",
    "            # Permute to the R(2+1)D required format: (N, C, T, H, W)\n",
    "            video_clips = video_clips.permute(0, 2, 1, 3, 4) \n",
    "        \n",
    "        # --- END PLACEHOLDER ---\n",
    "\n",
    "        # 4. Extract features for each clip\n",
    "        all_clip_features = []\n",
    "        with torch.no_grad():\n",
    "            # Check if video_clips is empty (only happens if total_frames < 16)\n",
    "            if video_clips.size(0) > 0:\n",
    "                for clip in video_clips:\n",
    "                    # Add batch dimension (1, 3, 16, 112, 112)\n",
    "                    clip = clip.unsqueeze(0) \n",
    "                    \n",
    "                    # Get the 2048-dimensional feature vector\n",
    "                    feature_vector = feature_extractor(clip).squeeze(0) # (2048)\n",
    "                    all_clip_features.append(feature_vector)\n",
    "\n",
    "        if not all_clip_features:\n",
    "            # If no clips were extracted (e.g., short video handled by warning), return a tensor of zeros\n",
    "            video_features_sequence = torch.zeros(1, VIDEO_FEATURE_DIM) \n",
    "        else:\n",
    "            # Concatenate all clip features to get the sequence tensor (num_clips, 2048)\n",
    "            video_features_sequence = torch.stack(all_clip_features) \n",
    "        \n",
    "        return video_features_sequence\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Loads the video features (either directly from file or by extraction) \n",
    "        and returns the feature tensor and the label.\n",
    "        \"\"\"\n",
    "        feature_filepath, label = self.samples[idx]\n",
    "        \n",
    "        if feature_filepath.endswith('.mp4'):\n",
    "            # If it's a raw video, extract features on the fly\n",
    "            features = self._extract_features_from_video(feature_filepath)\n",
    "        elif feature_filepath.endswith('.npy'):\n",
    "            features = torch.from_numpy(np.load(feature_filepath)).float()\n",
    "        elif feature_filepath.endswith('.pt'):\n",
    "            features = torch.load(feature_filepath).float()\n",
    "        else:\n",
    "            raise IOError(f\"Unsupported file type: {feature_filepath}\")\n",
    "        \n",
    "        # Ensure the feature tensor has the required shape (num_clips, feature_dim)\n",
    "        if features.dim() != 2:\n",
    "             print(f\"Warning: Features at {feature_filepath} have shape {features.shape}. Expected 2D. Skipping.\")\n",
    "             return self.__getitem__((idx + 1) % len(self)) \n",
    "        \n",
    "        return features, label\n",
    "\n",
    "# --- 2. Custom Collate Function for Padding ---\n",
    "\n",
    "def collate_fn(batch: List[Tuple[torch.Tensor, int]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Pads feature sequences to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    # Separate features and labels\n",
    "    features, labels = zip(*batch)\n",
    "    \n",
    "    # Get max sequence length in the batch\n",
    "    max_len = max(f.size(0) for f in features)\n",
    "    \n",
    "    # Pad all feature tensors to the max length\n",
    "    padded_features = []\n",
    "    for f in features:\n",
    "        pad_len = max_len - f.size(0)\n",
    "        # Pad with zeros at the end: (num_clips, feature_dim) -> (max_len, feature_dim)\n",
    "        padding = torch.zeros((pad_len, f.size(1)), dtype=f.dtype)\n",
    "        padded_features.append(torch.cat([f, padding], dim=0))\n",
    "        \n",
    "    # Stack tensors\n",
    "    padded_features = torch.stack(padded_features)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return padded_features, labels\n",
    "\n",
    "# --- 3. Example Usage to Integrate with Your Model ---\n",
    "\n",
    "def setup_data_pipeline(data_root: str, batch_size: int, num_workers: int = 0, shuffle: bool = True) -> DataLoader:\n",
    "    \"\"\"\n",
    "    Initializes the Dataset and DataLoader and updates the global NUM_CLASSES.\n",
    "    \"\"\"\n",
    "    global NUM_CLASSES\n",
    "    \n",
    "    # 1. Create Dataset\n",
    "    dataset = VideoClassificationDataset(data_root=data_root)\n",
    "    \n",
    "    # Update global NUM_CLASSES to reflect the actual data\n",
    "    new_num_classes = len(dataset.class_to_idx)\n",
    "    if NUM_CLASSES != new_num_classes:\n",
    "        print(f\"--- INFO: Updating global NUM_CLASSES from {NUM_CLASSES} to {new_num_classes} based on data folders. ---\")\n",
    "        NUM_CLASSES = new_num_classes\n",
    "        \n",
    "    # 2. Create DataLoader\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn, # Use the custom padding function\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e318cf",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5de413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention Mechanism for sequence data (LSTM outputs).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, attention_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # Bi-LSTM output is hidden_dim * 2\n",
    "        self.hidden_dim = hidden_dim * 2\n",
    "        \n",
    "        self.W_a = nn.Linear(self.hidden_dim, attention_dim, bias=False)\n",
    "        self.V_a = nn.Linear(attention_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, H):\n",
    "        \"\"\"\n",
    "        H: (batch_size, sequence_length, hidden_dim * num_directions) -> LSTM output sequence\n",
    "        \"\"\"\n",
    "        U = torch.tanh(self.W_a(H))\n",
    "        scores = self.V_a(U).squeeze(-1)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.sum(weights.unsqueeze(-1) * H, dim=1)\n",
    "        \n",
    "        return context, weights \n",
    "\n",
    "class SA_LSTM_Classification_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence-to-Classification Model using Bi-directional LSTM and Self-Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, video_feature_dim: int, hidden_dim: int, attention_dim: int, num_classes: int):\n",
    "        super(SA_LSTM_Classification_Model, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=video_feature_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=1, \n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        lstm_output_dim = hidden_dim * 2\n",
    "        \n",
    "        self.attention = SelfAttention(hidden_dim, attention_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(lstm_output_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, features: torch.Tensor):\n",
    "        # features shape: (batch_size, sequence_length, video_feature_dim)\n",
    "        H_out, _ = self.lstm(features)\n",
    "        context, _ = self.attention(H_out)\n",
    "        logits = self.classifier(context)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188eb89b",
   "metadata": {},
   "source": [
    "# Execution et tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadac715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, data_loader: DataLoader, criterion: nn.Module, device: torch.device) -> Tuple[float, float, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Calculates the average loss and accuracy over the given data loader.\n",
    "    Returns: avg_loss, accuracy, all_predictions, all_labels\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Log the expected number of samples for sanity check\n",
    "    expected_samples = len(data_loader.dataset)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
    "        for features, labels in data_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * features.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Store predictions and labels for detailed diagnostics\n",
    "            all_predictions.append(predicted.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            \n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    if total_samples == 0:\n",
    "        print(\"Warning: No samples were processed during evaluation.\")\n",
    "        return 0.0, 0.0, torch.empty(0, dtype=torch.long), torch.empty(0, dtype=torch.long)\n",
    "        \n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = 100 * correct_predictions / total_samples\n",
    "    \n",
    "    # Sanity check if the processed samples match the dataset size\n",
    "    if total_samples != expected_samples:\n",
    "         print(f\"CRITICAL WARNING: Processed {total_samples} samples, but dataset has {expected_samples}. Check for bad data!\")\n",
    "         \n",
    "    # Concatenate all stored predictions and labels\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    return avg_loss, accuracy, all_predictions, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ad3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_example(train_data_root: str, test_data_root: str):\n",
    "    \"\"\"\n",
    "    Simulates a training and evaluation run using separate data roots, \n",
    "    performing validation on the test set after every epoch.\n",
    "    \n",
    "    Includes model checkpointing based on best validation (test) accuracy.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running SA-LSTM Classification Pipeline Example ---\")\n",
    "    \n",
    "    Train_accs = []\n",
    "    Train_losss = []\n",
    "    Val_accs = []\n",
    "    Val_losss = []\n",
    "    LR = []\n",
    "\n",
    "    HIDDEN_DIM = 512\n",
    "    ATTENTION_DIM = 512\n",
    "    BATCH_SIZE = 4\n",
    "    # Set to 50 epochs to match the user's log context\n",
    "    NUM_EPOCHS = 50 \n",
    "\n",
    "    try:\n",
    "        # 1. Setup Training DataLoader\n",
    "        train_loader = setup_data_pipeline(train_data_root, BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        # 2. Setup Testing DataLoader (Validation Set)\n",
    "        test_loader = setup_data_pipeline(test_data_root, BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        global NUM_CLASSES, VIDEO_FEATURE_DIM \n",
    "        \n",
    "        # Get class index to name mapping from the test dataset for reporting\n",
    "        idx_to_class = {v: k for k, v in test_loader.dataset.class_to_idx.items()}\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Instantiate the model\n",
    "        model = SA_LSTM_Classification_Model(\n",
    "            video_feature_dim=VIDEO_FEATURE_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            attention_dim=ATTENTION_DIM,\n",
    "            num_classes=NUM_CLASSES \n",
    "        ).to(device)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-3)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # Reduce LR by half every 10 epochs\n",
    "\n",
    "        # --- Checkpointing Variables ---\n",
    "        best_test_accuracy = -1.0\n",
    "        # Updated path to match what was seen in the user's log output\n",
    "        checkpoint_path = 'best_model_checkpoint_full.pt' \n",
    "        # -------------------------------\n",
    "\n",
    "        # --- Training Loop ---\n",
    "        print(\"\\n--- Starting Training (Validation and Checkpointing enabled) ---\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # Training Phase\n",
    "            model.train() \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total_train_samples = 0\n",
    "            \n",
    "            for features, labels in train_loader: \n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                logits = model(features)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update training statistics\n",
    "                running_loss += loss.item() * features.size(0)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                running_corrects += (predicted == labels).sum().item()\n",
    "                total_train_samples += labels.size(0)\n",
    "            \n",
    "            # Training Metrics\n",
    "            train_loss = running_loss / total_train_samples\n",
    "            train_acc = 100 * running_corrects / total_train_samples\n",
    "            \n",
    "            # Validation (Test) Phase\n",
    "            test_loss, test_accuracy, _, _ = evaluate_model(model, test_loader, criterion, device) \n",
    "\n",
    "            # ---------------------------------\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            # Print Epoch Summary\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.6f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.6f}%, lr: {lr:3.6f}\")\n",
    "        \n",
    "            Train_accs.append(train_acc)\n",
    "            Train_losss.append(train_loss)\n",
    "            Val_accs.append(test_accuracy)\n",
    "            Val_losss.append(test_loss)\n",
    "            LR.append(lr)\n",
    "\n",
    "\n",
    "\n",
    "            # --- Checkpointing Logic ---\n",
    "            if test_accuracy > best_test_accuracy:\n",
    "                print(f\"    --> New best validation accuracy: {test_accuracy:.2f}%. Saving model to {checkpoint_path}\")\n",
    "                best_test_accuracy = test_accuracy\n",
    "                # Save only the model's learned parameters (state_dict)\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "            # ---------------------------\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "        # Final Output\n",
    "        print(\"\\n--- Training and Evaluation Process Complete ---\")\n",
    "        print(f\"--- Best model saved with Test Accuracy: {best_test_accuracy:.2f}% ---\")\n",
    "        return Train_accs, Train_losss, Val_accs, Val_losss, LR\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during data pipeline execution: {e}\")\n",
    "        # Add the helpful advice for CUDA errors\n",
    "        if \"CUDA error\" in str(e):\n",
    "             print(\"\\nSUGGESTION: The CUDA error often means the real error occurred earlier. Try running with the environment variable CUDA_LAUNCH_BLOCKING=1 to pinpoint the exact line of code that failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea3e22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running SA-LSTM Classification Pipeline Example ---\n",
      "Found 6513 files across 20 classes.\n",
      "Class Mapping: {'advertisement': 0, 'animals_pets': 1, 'animation': 2, 'beauty_fashion': 3, 'cooking': 4, 'documentary': 5, 'education': 6, 'food_drink': 7, 'gaming': 8, 'howto': 9, 'kids_family': 10, 'movie_comedy': 11, 'music': 12, 'news_events_politics': 13, 'people': 14, 'science_technology': 15, 'sports_actions': 16, 'travel': 17, 'tv shows': 18, 'vehicles_autos': 19}\n",
      "--- INFO: Updating global NUM_CLASSES from 2 to 20 based on data folders. ---\n",
      "Found 497 files across 20 classes.\n",
      "Class Mapping: {'advertisement': 0, 'animals_pets': 1, 'animation': 2, 'beauty_fashion': 3, 'cooking': 4, 'documentary': 5, 'education': 6, 'food_drink': 7, 'gaming': 8, 'howto': 9, 'kids_family': 10, 'movie_comedy': 11, 'music': 12, 'news_events_politics': 13, 'people': 14, 'science_technology': 15, 'sports_actions': 16, 'travel': 17, 'tv shows': 18, 'vehicles_autos': 19}\n",
      "\n",
      "--- Starting Training (Validation and Checkpointing enabled) ---\n",
      "Epoch 1/50 | Train Loss: 2.9857, Train Acc: 9.196991% | Test Loss: 2.9711, Test Acc: 9.054326%, lr: 0.001000\n",
      "    --> New best validation accuracy: 9.05%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 2/50 | Train Loss: 2.9842, Train Acc: 9.596192% | Test Loss: 2.9678, Test Acc: 10.060362%, lr: 0.001000\n",
      "    --> New best validation accuracy: 10.06%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 3/50 | Train Loss: 2.9880, Train Acc: 9.749731% | Test Loss: 2.9335, Test Acc: 11.468813%, lr: 0.001000\n",
      "    --> New best validation accuracy: 11.47%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 4/50 | Train Loss: 3.0125, Train Acc: 9.043452% | Test Loss: 2.9281, Test Acc: 8.853119%, lr: 0.001000\n",
      "Epoch 5/50 | Train Loss: 3.0108, Train Acc: 9.473361% | Test Loss: 3.0066, Test Acc: 16.096579%, lr: 0.001000\n",
      "    --> New best validation accuracy: 16.10%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 6/50 | Train Loss: 2.9931, Train Acc: 10.041456% | Test Loss: 2.9712, Test Acc: 8.249497%, lr: 0.001000\n",
      "Epoch 7/50 | Train Loss: 2.9912, Train Acc: 10.041456% | Test Loss: 2.9035, Test Acc: 9.657948%, lr: 0.001000\n",
      "Epoch 8/50 | Train Loss: 2.9962, Train Acc: 9.657608% | Test Loss: 2.9194, Test Acc: 12.474849%, lr: 0.001000\n",
      "Epoch 9/50 | Train Loss: 2.9825, Train Acc: 10.056809% | Test Loss: 2.9462, Test Acc: 11.267606%, lr: 0.001000\n",
      "Epoch 10/50 | Train Loss: 2.9900, Train Acc: 9.688316% | Test Loss: 2.9206, Test Acc: 14.486922%, lr: 0.001000\n",
      "Epoch 11/50 | Train Loss: 2.9103, Train Acc: 11.438661% | Test Loss: 2.8663, Test Acc: 10.060362%, lr: 0.000500\n",
      "Epoch 12/50 | Train Loss: 2.8134, Train Acc: 16.367265% | Test Loss: 2.6085, Test Acc: 24.547284%, lr: 0.000500\n",
      "    --> New best validation accuracy: 24.55%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 13/50 | Train Loss: 2.5125, Train Acc: 26.715799% | Test Loss: 2.2851, Test Acc: 32.997988%, lr: 0.000500\n",
      "    --> New best validation accuracy: 33.00%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 14/50 | Train Loss: 2.3897, Train Acc: 30.569630% | Test Loss: 2.2176, Test Acc: 34.406439%, lr: 0.000500\n",
      "    --> New best validation accuracy: 34.41%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 15/50 | Train Loss: 2.3336, Train Acc: 31.997543% | Test Loss: 2.2031, Test Acc: 33.802817%, lr: 0.000500\n",
      "Epoch 16/50 | Train Loss: 2.2934, Train Acc: 33.287272% | Test Loss: 2.1441, Test Acc: 35.814889%, lr: 0.000500\n",
      "    --> New best validation accuracy: 35.81%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 17/50 | Train Loss: 2.2746, Train Acc: 32.964840% | Test Loss: 2.1029, Test Acc: 39.034205%, lr: 0.000500\n",
      "    --> New best validation accuracy: 39.03%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 18/50 | Train Loss: 2.2472, Train Acc: 34.377399% | Test Loss: 2.2334, Test Acc: 36.016097%, lr: 0.000500\n",
      "Epoch 19/50 | Train Loss: 2.2403, Train Acc: 34.239214% | Test Loss: 2.0812, Test Acc: 38.229376%, lr: 0.000500\n",
      "Epoch 20/50 | Train Loss: 2.2344, Train Acc: 34.807308% | Test Loss: 2.1587, Test Acc: 37.424547%, lr: 0.000500\n",
      "Epoch 21/50 | Train Loss: 2.1253, Train Acc: 37.755259% | Test Loss: 2.1367, Test Acc: 36.418511%, lr: 0.000250\n",
      "Epoch 22/50 | Train Loss: 2.0961, Train Acc: 37.924152% | Test Loss: 1.9921, Test Acc: 41.247485%, lr: 0.000250\n",
      "    --> New best validation accuracy: 41.25%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 23/50 | Train Loss: 2.0892, Train Acc: 38.645785% | Test Loss: 2.0355, Test Acc: 39.839034%, lr: 0.000250\n",
      "Epoch 24/50 | Train Loss: 2.0846, Train Acc: 39.213880% | Test Loss: 2.0160, Test Acc: 38.832998%, lr: 0.000250\n",
      "Epoch 25/50 | Train Loss: 2.0685, Train Acc: 39.213880% | Test Loss: 2.0170, Test Acc: 40.845070%, lr: 0.000250\n",
      "Epoch 26/50 | Train Loss: 2.0652, Train Acc: 39.643789% | Test Loss: 1.9958, Test Acc: 40.845070%, lr: 0.000250\n",
      "Epoch 27/50 | Train Loss: 2.0762, Train Acc: 39.689851% | Test Loss: 2.0455, Test Acc: 40.241449%, lr: 0.000250\n",
      "Epoch 28/50 | Train Loss: 2.0634, Train Acc: 39.428835% | Test Loss: 2.0108, Test Acc: 39.034205%, lr: 0.000250\n",
      "Epoch 29/50 | Train Loss: 2.0545, Train Acc: 39.689851% | Test Loss: 2.0448, Test Acc: 39.637827%, lr: 0.000250\n",
      "Epoch 30/50 | Train Loss: 2.0665, Train Acc: 39.643789% | Test Loss: 2.0207, Test Acc: 41.649899%, lr: 0.000250\n",
      "    --> New best validation accuracy: 41.65%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 31/50 | Train Loss: 1.9824, Train Acc: 42.392139% | Test Loss: 1.9528, Test Acc: 42.052314%, lr: 0.000125\n",
      "    --> New best validation accuracy: 42.05%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 32/50 | Train Loss: 1.9618, Train Acc: 42.484262% | Test Loss: 1.9845, Test Acc: 41.649899%, lr: 0.000125\n",
      "Epoch 33/50 | Train Loss: 1.9555, Train Acc: 42.868110% | Test Loss: 1.9326, Test Acc: 43.058350%, lr: 0.000125\n",
      "    --> New best validation accuracy: 43.06%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 34/50 | Train Loss: 1.9275, Train Acc: 43.605097% | Test Loss: 1.9713, Test Acc: 40.643863%, lr: 0.000125\n",
      "Epoch 35/50 | Train Loss: 1.9238, Train Acc: 43.758637% | Test Loss: 1.9034, Test Acc: 42.454728%, lr: 0.000125\n",
      "Epoch 36/50 | Train Loss: 1.8893, Train Acc: 44.418855% | Test Loss: 1.8919, Test Acc: 41.851107%, lr: 0.000125\n",
      "Epoch 37/50 | Train Loss: 1.8844, Train Acc: 45.140488% | Test Loss: 1.8801, Test Acc: 44.064386%, lr: 0.000125\n",
      "    --> New best validation accuracy: 44.06%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 38/50 | Train Loss: 1.8593, Train Acc: 45.324735% | Test Loss: 1.9122, Test Acc: 42.857143%, lr: 0.000125\n",
      "Epoch 39/50 | Train Loss: 1.8284, Train Acc: 46.844772% | Test Loss: 1.8871, Test Acc: 44.466801%, lr: 0.000125\n",
      "    --> New best validation accuracy: 44.47%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 40/50 | Train Loss: 1.8309, Train Acc: 46.445570% | Test Loss: 1.8717, Test Acc: 44.466801%, lr: 0.000125\n",
      "Epoch 41/50 | Train Loss: 1.7379, Train Acc: 49.071089% | Test Loss: 1.8379, Test Acc: 44.466801%, lr: 0.000063\n",
      "Epoch 42/50 | Train Loss: 1.7079, Train Acc: 50.545064% | Test Loss: 1.8637, Test Acc: 45.472837%, lr: 0.000063\n",
      "    --> New best validation accuracy: 45.47%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 43/50 | Train Loss: 1.6860, Train Acc: 50.621833% | Test Loss: 1.8336, Test Acc: 44.064386%, lr: 0.000063\n",
      "Epoch 44/50 | Train Loss: 1.6781, Train Acc: 50.959619% | Test Loss: 1.8589, Test Acc: 43.460765%, lr: 0.000063\n",
      "Epoch 45/50 | Train Loss: 1.6534, Train Acc: 52.003685% | Test Loss: 1.8372, Test Acc: 46.277666%, lr: 0.000063\n",
      "    --> New best validation accuracy: 46.28%. Saving model to best_model_checkpoint_full.pt\n",
      "Epoch 46/50 | Train Loss: 1.6305, Train Acc: 51.957623% | Test Loss: 1.8214, Test Acc: 44.869215%, lr: 0.000063\n",
      "Epoch 47/50 | Train Loss: 1.6115, Train Acc: 52.740673% | Test Loss: 1.8563, Test Acc: 45.271630%, lr: 0.000063\n",
      "Epoch 48/50 | Train Loss: 1.6017, Train Acc: 53.585137% | Test Loss: 1.8127, Test Acc: 43.259557%, lr: 0.000063\n",
      "Epoch 49/50 | Train Loss: 1.5749, Train Acc: 53.554430% | Test Loss: 1.8318, Test Acc: 44.668008%, lr: 0.000063\n",
      "Epoch 50/50 | Train Loss: 1.5590, Train Acc: 53.600491% | Test Loss: 1.8346, Test Acc: 44.466801%, lr: 0.000063\n",
      "\n",
      "--- Training and Evaluation Process Complete ---\n",
      "--- Best model saved with Test Accuracy: 46.28% ---\n"
     ]
    }
   ],
   "source": [
    "Train_accs, Train_losss, Val_accs, Val_losss, LR = run_classification_example('./TrainValFeatures/Train', './TrainValFeatures/Val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
